{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    #initilize neuron with activation type, number of inputs, learning rate, and possibly with set weights\n",
    "    def __init__(self,acfunc, input_num, lr, weights=None):\n",
    "        # print('constructor')\n",
    "        self.acfunc = acfunc\n",
    "        if weights is None:\n",
    "            self.weights = np.random.random([input_num + 1]) # including bias\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        self.lr = lr\n",
    "\n",
    "    #This method returns the activation of the net\n",
    "    def activate(self,net):\n",
    "        # print('activate')\n",
    "        if self.acfunc == 0: # linear\n",
    "            return net\n",
    "        else: # logistic\n",
    "            return 1/(1+np.exp(-net))\n",
    "\n",
    "    #Calculate the output of the neuron should save the input and output for back-propagation.   \n",
    "    def calculate(self,Input):\n",
    "        self.ins = np.append(Input, 1.0)\n",
    "        net = np.sum(self.ins*self.weights, axis=1)\n",
    "        self.outs = round(self.activate(net), 3)\n",
    "        return self.outs\n",
    "\n",
    "    #This method returns the derivative of the activation function with respect to the net   \n",
    "    def activationderivative(self):\n",
    "        # print('activationderivative')\n",
    "        if self.acfunc == 0: # activation function is linear\n",
    "            return 1\n",
    "        else: # activation function is sigmoid\n",
    "            return self.outs * (1-self.outs)\n",
    "    \n",
    "    #This method calculates the partial derivative for each weight and returns the delta*w to be used in the previous layer\n",
    "    def calcpartialderivative(self, wtimesdelta):\n",
    "        # print('calcpartialderivative')\n",
    "        self.deltapartw = wtimesdelta.reshape(-1,1) * self.activationderivative() * self.ins\n",
    "        wd = wtimesdelta.reshape(-1,1) * self.activationderivative() * self.weights[:-1] # wtimesdelta of current cell\n",
    "        return wd\n",
    "    \n",
    "    #Simply update the weights using the partial derivatives and the leranring weight\n",
    "    def updateweight(self):\n",
    "        # print('updateweight')\n",
    "        self.weights = self.weights - self.lr * self.deltapartw\n",
    "        return 0\n",
    "\n",
    "    def updateweight_conv(self, NEWweights):\n",
    "        # used in convolution layer, to synchronize weights\n",
    "        self.weights = NEWweights\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    #initialize with the number of neurons in the layer, their activation,the input size, the leraning rate and a 2d matrix of weights (or else initilize randomly)\n",
    "    def __init__(self,numOfNeurons, acfunc, input_num, lr, weights=None):\n",
    "        # print('constructor')\n",
    "        #self.d = input_num\n",
    "        self.layer = []\n",
    "        if weights is None:\n",
    "            for i in range(numOfNeurons):\n",
    "                self.layer.append(Neuron(acfunc, input_num, lr))\n",
    "        else:\n",
    "            for i in range(numOfNeurons):\n",
    "                self.layer.append(Neuron(acfunc, input_num, lr, weights[i]))\n",
    "        \n",
    "    #calcualte the output of all the neurons in the layer and return a vector with those values (go through the neurons and call the calcualte() method)      \n",
    "    def calculate(self, Input):\n",
    "        # print('calculate')\n",
    "        output = []\n",
    "        #Input = np.append(Input, 1)\n",
    "        for cell in self.layer:\n",
    "            output.append(cell.calculate(Input))\n",
    "\n",
    "        return output\n",
    "            \n",
    "    #given the next layer's w*delta, should run through the neurons calling calcpartialderivative() for each (with the correct value), sum up its ownw*delta, and then update the wieghts (using the updateweight() method). I should return the sum of w*delta.          \n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        #wtimesdeltaNEW = np.zeros(self.d)\n",
    "        wtimesdeltaNEW = 0\n",
    "        for cell, wd in zip(self.layer, wtimesdelta):\n",
    "            wtimesdeltaNEW += cell.calcpartialderivative(wd)\n",
    "            #print(cell.calcpartialderivative(wd))\n",
    "            self.weights = self.weights - self.lr * self.deltapartw\n",
    "            cell.updateweight()\n",
    "\n",
    "        return wtimesdeltaNEW"
   ]
  },
  {
   "source": [
    "class convolutionalLayer:\n",
    "    def __init__(self, KernelShape:int, KernelNum:int, acfunc:int, InputShape:int, lr:float, weights=None):\n",
    "        # the KernelShape represents edge lenght of square\n",
    "        self.acfunc = acfunc\n",
    "        self.InputShape = InputShape\n",
    "        self.OutputShape = InputShape-KernelShape+1 # equales cell shape stride is always 1\n",
    "        if weights is None:\n",
    "            self.weights = np.random.random([InputShape + 1, KernelNum])\n",
    "            # kernel is flattened in row order (\"C\"), with bias at last position\n",
    "            # only one layer of cell, while weights have multiple layer\n",
    "        else:\n",
    "            self.weights = np.hstack((weights, np.ones([KernelNum,1]))) # add 1s to the end\n",
    "        self.layer = []\n",
    "        for i in range(self.OutputShape*self.OutputShape):\n",
    "            # number of neurons equals number of outputs\n",
    "            self.layer.append([Neuron(acfunc, InputShape, lr, self.weights)])\n",
    "\n",
    "    def activationderivative(self):\n",
    "        # print('activationderivative')\n",
    "        if self.acfunc == 0: # activation function is linear\n",
    "            return np.ones_like(self.conv_output)\n",
    "        else: # activation function is sigmoid\n",
    "            return self.conv_output * (np.ones_like(self.conv_output)-self.conv_output)\n",
    "\n",
    "    def calculate(self, Input):\n",
    "        self.conv_input = []\n",
    "        for layer1 in np.lib.stride_tricks.sliding_window_view(Input, window_shape=(2,2)):\n",
    "            for layer2 in layer1:\n",
    "                self.conv_input.append(layer2.flatten()) # input is flatten to 1d\n",
    "        self.conv_input = np.array(self.conv_input)\n",
    "        self.conv_output = []\n",
    "        for index, cell in enumerate(self.layer):\n",
    "            self.conv_output.append(cell.calculate(self.conv_input[index]))\n",
    "        self.conv_output = np.array(self.conv_output)\n",
    "        outs = self.conv_output.reshape(self.OutputShape, self.OutputShape) # change back to 2d\n",
    "        return outs\n",
    "\n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        # update weights\n",
    "        wtimesdeltaNEW = []\n",
    "        deltapartw = []\n",
    "        for wd, cell in zip(wtimesdelta.reshape(-1, self.InputShape).T, self.layer):\n",
    "            wtimesdeltaNEW.append(cell.calcpartialderivative(wd))\n",
    "            deltapartw.append(cell.deltapartw)\n",
    "        self.weights = self.weights - self.lr * deltapartw\n",
    "        for cell in self.layer:\n",
    "            cell.updateweight_conv(self.weights)\n",
    "        # calculate w*delta of former layer\n",
    "        wtimesdeltaNEW = wtimesdelta * self.activationderivative() * self.weights[:,:-1]\n",
    "        return wtimesdeltaNEW"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, KernelShape:int, InputShape:int):\n",
    "        #self.InputShape = InputShape\n",
    "        self.OutputShape = InputShape/KernelShape # stride = kernel size\n",
    "\n",
    "    def calculate(self, Input):\n",
    "        self.conv_input = []\n",
    "        for layer1 in np.lib.stride_tricks.sliding_window_view(Input, window_shape=(self.OutputShape,self.OutputShape))[0::self.OutputShape,0::self.OutputShape]: # use index to perform stride\n",
    "            for layer2 in layer1:\n",
    "                self.conv_input.append(layer2.flatten()) # input is flatten to 1d\n",
    "        self.conv_input = np.array(self.conv_input)\n",
    "        self.conv_output = []\n",
    "        self.activationderivative = [] # record the position of max (which has activation derivative of 1, other positions are 0), used for back-propagation\n",
    "        for ins in self.conv_input:\n",
    "            # get the max index\n",
    "            idx = np.zeros_like(ins)\n",
    "            idx[ins.argmax()] = 1\n",
    "            self.activationderivative.append(idx)\n",
    "            # get the max and append to output\n",
    "            self.conv_output.append(np.amax(ins))\n",
    "        self.activationderivative = np.array(self.activationderivative)\n",
    "        self.conv_output = np.array(self.conv_output)\n",
    "        outs = self.conv_output.reshape(self.OutputShape, self.OutputShape) # change back to 2d\n",
    "        return outs\n",
    "\n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        wtimesdeltaNEW = wtimesdelta * self.activationderivative\n",
    "        return wtimesdeltaNEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __init__(self, InputShape:int):\n",
    "        # flatten the input row wise\n",
    "        self.InputShape = InputShape\n",
    "\n",
    "    def calculate(self, Input):\n",
    "        return Input.flatten()\n",
    "\n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        return wtimesdelta.reshape(self.InputShape, self.InputShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    #initialize with the number of layers, number of neurons in each layer (vector), input size, activation (for each layer), the loss function, the learning rate and a 3d matrix of weights weights (or else initialize randomly)\n",
    "    def __init__(self, InputSize:int, lossfunc, lr):\n",
    "        # Input is the edge length of input\n",
    "        self.InputSize = InputSize # the initial input size, change after every layer\n",
    "        self.lossfunc = lossfunc\n",
    "        self.lr = lr\n",
    "    \n",
    "    def addLayer()\n",
    "    #Given an input, calculate the output (using the layers calculate() method)\n",
    "    def calculate(self,Input):\n",
    "        # print('constructor')\n",
    "        for layer in self.network:\n",
    "            output = layer.calculate(Input)\n",
    "            #print(\"layer output\", output)\n",
    "            Input = output\n",
    "\n",
    "        return output\n",
    "        \n",
    "    #Given a predicted output and ground truth output simply return the loss (depending on the loss function)\n",
    "    def calculateloss(self,yp,y):\n",
    "        # print('calculate')\n",
    "        if self.loss == 0:# sum of squared errors\n",
    "            return 0.5 * np.sum(np.square(yp-y))\n",
    "        else: # binary cross entropy\n",
    "            return np.sum(-(y.dot(np.log(yp)) + (np.full_like(y, 1)-y).dot(np.log(np.full_like(yp, 1)-yp))))\n",
    "        \n",
    "    \n",
    "    #Given a predicted output and ground truth output simply return the derivative of the loss (depending on the loss function)        \n",
    "    def lossderiv(self,yp,y):\n",
    "        if self.loss == 0: # sum of squared errors\n",
    "            #return np.abs(y-yp)\n",
    "            return yp - y\n",
    "        else: # binary cross entropy\n",
    "            #return -y/yp + (np.full_like(y,1)-y)/(np.full_like(yp,1)-yp)\n",
    "            return (yp - y) / (yp - yp * yp)\n",
    "    \n",
    "    #Given a single input and desired output preform one step of backpropagation (including a forward pass, getting the derivative of the loss, and then calling calcwdeltas for layers with the right values         \n",
    "    def train(self,x,y):\n",
    "        yp = self.calculate(x) # predicted result\n",
    "        E = self.calculateloss(yp, y) # loss\n",
    "        dE = self.lossderiv(yp, y) # delta loss\n",
    "        for layer in reversed(self.network):\n",
    "            dE = layer.calcwdeltas(dE)\n",
    "        return yp, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoExample(w,x,y):\n",
    "    # numOfLayers,numOfNeurons, inputSize, activation, loss, lr, weights\n",
    "    model = NeuralNetwork(2, [2,2], 2, [1,1], 0, 0.5, w)\n",
    "    ypredict, E = model.train(x,y)\n",
    "    for layer in model.network:\n",
    "        for cell in layer.layer:\n",
    "            print(cell.weights)\n",
    "\n",
    "    print(ypredict, E)\n",
    "    return 0"
   ]
  },
  {
   "source": [
    "# old output\n",
    "# [0.14955069 0.19910139 0.34101389]\n",
    "# [0.24949899 0.29899797 0.33997971]\n",
    "# [0.3589151  0.40863797 0.53071687]\n",
    "# [0.48871011 0.53863395 0.5809614 ]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0554265  0.06235482]\n[-0.0190386  -0.02094246]\n[0.00131734 0.00175645]\n[0.00249086 0.00298903]\n[0.14978044 0.19956089 0.34560887]\n[0.24975091 0.29950183 0.34501828]\n[0.3589151  0.40863797 0.53071687]\n[0.51128989 0.56136605 0.6190386 ]\n[0.751, 0.773] 0.298085\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# old output\n",
    "w = np.array([[[.15,.2,.35],[.25,.3,.35]],[[.4,.45,.6],[.5,.55,.6]]])\n",
    "x = np.array([0.05,0.1])\n",
    "y = np.array([0.01,0.99])\n",
    "DoExample(w,x,y)"
   ]
  }
 ]
}