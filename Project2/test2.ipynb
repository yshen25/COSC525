{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 29 22:27:15 2021\n",
    "\n",
    "@author: priyojitDas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "For this entire file there are a few constants:\n",
    "activation:\n",
    "0 - linear\n",
    "1 - logistic (only one supported)\n",
    "loss:\n",
    "0 - sum of square errors\n",
    "1 - binary cross entropy\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # This function applies sigmoid function to the input and returns the output \n",
    "    return 1./(1+np.exp(-x))\n",
    "\n",
    "# A class which represents a single neuron\n",
    "class Neuron:\n",
    "    #initilize neuron with activation type, number of inputs, learning rate, and possibly with set weights\n",
    "    activation_f = None # store activation type of the neuron - linear or logistic (sigmoid); str\n",
    "    l_rate = None # store learning rate; float\n",
    "    W = None # weight vector of the neuron; numpy array shape (number_of_nodes_in presvious_layer+1,); +1 for bias node\n",
    "    n_input = None # store input to the neuron for future use in backpropagation\n",
    "    n_output = None # store output of the neuron for future use in backpropagation\n",
    "    dLdW = 0 # dLoss/dW\n",
    "    dLdb = 0 # dLoss/db\n",
    "    \n",
    "    def __init__(self, activation, lr, weights):\n",
    "        #print('constructor') \n",
    "        self.activation_f = activation\n",
    "        #self.n_dim = input_num\n",
    "        self.l_rate = lr\n",
    "        self.W = weights\n",
    "        \n",
    "    #This method returns the activation of the net\n",
    "    def activate(self,net):\n",
    "        #print('activate')\n",
    "        if self.activation_f == 'linear':\n",
    "            # if activation type is linear, return the input to this function\n",
    "            return net\n",
    "        elif self.activation_f == 'logistic':\n",
    "            # if activation type is logistic, apply sigmoid function to the input and return the output\n",
    "            return 1./(1+np.exp(-net))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    #Calculate the output of the neuron should save the input and output for back-propagation.   \n",
    "    def calculate(self,input):\n",
    "        #print('calculate')\n",
    "        self.n_input = input\n",
    "        self.n_output = self.activate(np.sum(self.W[:-1]*input) + self.W[-1])\n",
    "        return self.n_output\n",
    "\n",
    "    #This method returns the derivative of the activation function with respect to the net   \n",
    "    def activationderivative(self):\n",
    "        #print('activationderivative')\n",
    "        if self.activation_f == 'linear':\n",
    "            return 1\n",
    "        elif self.activation_f == 'logistic':\n",
    "            return self.n_output*(1-self.n_output)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #This method calculates the partial derivative for each weight and returns the delta*w to be used in the previous layer\n",
    "    def calcpartialderivative(self, wtimesdelta):\n",
    "        #print('calcpartialderivative')\n",
    "        delta = wtimesdelta * self.activationderivative() # calculate delta\n",
    "        self.dLdW += self.n_input * delta  # for each iteration adds up dLdW of all samples\n",
    "        self.dLdb += delta # for each iteration adds up dLdb of all samples\n",
    "        return self.W[:-1] * delta # return w*delta\n",
    "    \n",
    "    #Simply update the weights using the partial derivatives and the leranring weight\n",
    "    def updateweight(self):\n",
    "        #print('updateweight')\n",
    "        self.W[:-1] -= self.l_rate * self.dLdW # update neuron weights \n",
    "        self.W[-1] -= self.l_rate * self.dLdb # update bias weight\n",
    "        self.dLdW = 0 # set dLdW and dLdb to 0 after weight update\n",
    "        self.dLdb = 0\n",
    "        \n",
    "#A fully connected layer        \n",
    "class FullyConnected:\n",
    "    #initialize with the number of neurons in the layer, their activation,the input size, the leraning rate and a 2d matrix of weights (or else initilize randomly)\n",
    "    n_neuron = None # number of neuron in the layer; integer\n",
    "    activation_l = None # activation function type for the neurons of the layer; str \n",
    "    l_rate = None # learing rate\n",
    "    W = None # weight matrix; numpy 2D matrix of shape (number_of_nodes_in_current_layer,number_of_nodes_in_previous_layer+1; +1 for bias)\n",
    "    neurons_ = None # store the neurons; list; length: n_neuron; each item represents a neuron object\n",
    "    \n",
    "    def __init__(self, numOfNeurons, activation, lr, weights=None):\n",
    "        #print('constructor')\n",
    "        self.n_neuron = numOfNeurons\n",
    "        self.activation_l = activation\n",
    "        self.l_rate = lr\n",
    "        self.W = weights\n",
    "        \n",
    "        # creates 'n_neuron' number of neurons and stores them in a list\n",
    "        self.neurons_ = [Neuron(self.activation_l,self.l_rate,self.W[i]) for i in range(self.n_neuron)]\n",
    "        \n",
    "    #calcualte the output of all the neurons in the layer and return a vector with those values (go through the neurons and call the calcualte() method)      \n",
    "    def calculate(self, input):\n",
    "        #print('calculate') \n",
    "        layer_f = np.array([neuron.calculate(input) for neuron in self.neurons_])\n",
    "        return layer_f\n",
    "        \n",
    "    #given the next layer's w*delta, should run through the neurons calling calcpartialderivative() for each (with the correct value), sum up its ownw*delta, and then update the wieghts (using the updateweight() method). I should return the sum of w*delta.          \n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        #print('calcwdeltas')\n",
    "        s_wdelta = 0\n",
    "        for i,neuron in enumerate(self.neurons_):\n",
    "            s_wdelta += neuron.calcpartialderivative(wtimesdelta[i])\n",
    "            #neuron.updateweight()\n",
    "        return s_wdelta\n",
    "           \n",
    "        \n",
    "#An entire neural network        \n",
    "class NeuralNetwork:\n",
    "    #initialize with the number of layers, number of neurons in each layer (vector), input size, activation (for each layer), the loss function, the learning rate and a 3d matrix of weights weights (or else initialize randomly) \n",
    "    n_layer = None # number of layers in the neural network (hidden + output); integer\n",
    "    n_layer_neurons = None # number of neurons in each layer; list\n",
    "    n_dim = None # dimension of the inputs or number of input nodes; integer \n",
    "    activation_t = None # activation function for each of the fully connected layers; list\n",
    "    loss_t = None # loss function type: sum of squared error or binary cross entropy; str\n",
    "    l_rate = None # learning rate; float\n",
    "    W = None\n",
    "    layers_ = None # stores the neural network layers; list; length: n_layer; each item represents a fully connected layer object\n",
    "    \n",
    "    def __init__(self,numOfLayers,numOfNeurons, inputSize, activation, loss, lr, weights=None):\n",
    "        #print('constructor')\n",
    "        self.n_layer = numOfLayers\n",
    "        self.n_layer_neurons = numOfNeurons\n",
    "        self.n_dim = inputSize\n",
    "        self.activation_t = activation\n",
    "        self.loss_t = loss\n",
    "        self.l_rate = lr\n",
    "        \n",
    "        if weights is None:\n",
    "            # if weights are not supplied, a weight list is created where each of the elements is a numpy 2D matrix of shape (number_of_nodes_in_current_layer,number_of_nodes_in_previous_layer+1; +1 for bias node) associted with a fully connected layer \n",
    "            temp_ = [self.n_dim] + self.n_layer_neurons\n",
    "            #print(temp_)\n",
    "            self.W = [np.random.normal(0,1*temp_[i]**-0.5,[temp_[i+1],temp_[i]+1]) for i in range(len(temp_)-1)]\n",
    "            #self.W = [np.random.randn(temp_[i+1],temp_[i]+1) for i in range(len(temp_)-1)]\n",
    "            #print(self.W)\n",
    "        else:\n",
    "            self.W = weights\n",
    "            \n",
    "        # creates 'n_layer' number of fully connected layers and stores them in a list  \n",
    "        self.layers_ = [FullyConnected(self.n_layer_neurons[i],self.activation_t[i],self.l_rate,self.W[i]) for i in range(self.n_layer)]\n",
    "    \n",
    "    #Given an input, calculate the output (using the layers calculate() method)\n",
    "    def calculate(self,input):\n",
    "        #print('constructor')\n",
    "        # performs a forward pass through the neural network and return predicted output\n",
    "        for layer in self.layers_:\n",
    "            input = layer.calculate(input)\n",
    "        return input\n",
    "            \n",
    "    #Given a predicted output and ground truth output simply return the loss (depending on the loss function)\n",
    "    def calculateloss(self,yp,y):\n",
    "        #print('calculate')\n",
    "        # calculates loss for a sample by comparing predicted output with ground truth after forward pass and return it  \n",
    "        if self.loss_t == 'sse':\n",
    "            return 0.5 * np.sum((yp-y) ** 2)\n",
    "        elif self.loss_t == 'bce':\n",
    "            return -(y*np.log(yp)+(1-y)*np.log(1-yp))\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    #Given a predicted output and ground truth output simply return the derivative of the loss (depending on the loss function)        \n",
    "    def lossderiv(self,yp,y):\n",
    "        if self.loss_t == 'sse':\n",
    "            return (yp - y)\n",
    "        elif self.loss_t == 'bce':\n",
    "            return (yp - y) / (yp - yp * yp)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #Given a single input and desired output preform one step of backpropagation (including a forward pass, getting the derivative of the loss, and then calling calcwdeltas for layers with the right values         \n",
    "    def train(self,x,y,iterations,printWeight=False):\n",
    "        #print('train')\n",
    "        error_l = [] # store neural network training error for each iteration\n",
    "        for it in range(iterations):\n",
    "            error = 0\n",
    "            for i in range(x.shape[0]): # pass through each sample\n",
    "                yp  = self.calculate(x[i]) # perform forward pass and stores the predicted output in yp \n",
    "                loss_v = self.calculateloss(yp,y[i]) # calculate loss\n",
    "                error += loss_v # add the loss for each sample\n",
    "                wdelta = self.lossderiv(yp,y[i]) # calculate derivative of the loss\n",
    "                for layer in self.layers_[::-1]: # perform backpropagation \n",
    "                    wdelta = layer.calcwdeltas(wdelta)\n",
    "            for layer in self.layers_[::-1]: # update weight based on backpropagation\n",
    "                for neuron in layer.neurons_:\n",
    "                    neuron.updateweight()\n",
    "            #print(\"Epoch\",it+1,error)\n",
    "            error_l.append(error/x.shape[0])\n",
    "        if printWeight:\n",
    "            for layer in self.layers_[::-1]: # update weight based on backpropagation\n",
    "                for neuron in layer.neurons_:\n",
    "                    print(neuron.W)\n",
    "        print(\"Final Output:\")\n",
    "        for i in range(x.shape[0]):\n",
    "            print(\"Input:\",x[i],\"Output:\",self.calculate(x[i]))\n",
    "        return error_l\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    if (len(sys.argv)<2):\n",
    "        print('a good place to test different parts of your code')\n",
    "        \n",
    "    elif (sys.argv[1]=='example'):\n",
    "        print('run example from class (single step)')\n",
    "        w = np.array([[[.15,.2,.35],[.25,.3,.35]],[[.4,.45,.6],[.5,.55,.6]]])\n",
    "        x = np.array([[0.05,0.1]])\n",
    "        y = np.array([[0.01,0.99]])\n",
    "        \n",
    "        nn = NeuralNetwork(numOfLayers=2,numOfNeurons=[2,2],inputSize=2,activation=['logistic','logistic'],loss='sse',lr=0.5,weights=w)\n",
    "        t_error = nn.train(x,y,iterations=1,printWeight=True)\n",
    "        \n",
    "        for lrate in [0.5]:\n",
    "            print(\"*\" * 50)\n",
    "            print(\"Learning Rate: %f\" % (lrate))\n",
    "            nn = NeuralNetwork(numOfLayers=2,numOfNeurons=[2,2],inputSize=2,activation=['logistic','logistic'],loss='sse',lr=lrate,weights=w)\n",
    "            t_error = nn.train(x,y,iterations=100)\n",
    "            plt.plot(t_error,label='Learing Rate: %f' % (lrate))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    elif(sys.argv[1]=='and'):\n",
    "        print('learn and')\n",
    "        w = None\n",
    "        x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "        y = np.array([[0],[0],[0],[1]])\n",
    "        \n",
    "        for lrate in [0.5,0.1,0.05,0.01,0.005]:\n",
    "            nn = NeuralNetwork(numOfLayers=1,numOfNeurons=[1],inputSize=2,activation=['logistic'],loss='bce',lr=lrate,weights=w)\n",
    "            print(\"*\" * 50)\n",
    "            print(\"Learning Rate: %f\" % (lrate))\n",
    "            t_error = nn.train(x,y,iterations=10000)\n",
    "            plt.plot(t_error,label='Learing Rate: %f' % (lrate))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    elif(sys.argv[1]=='xor'):\n",
    "        print('learn xor')\n",
    "        w = None\n",
    "        x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "        y = np.array([[0],[1],[1],[0]])\n",
    "        \n",
    "        print('Single preceptron ......')\n",
    "        for lrate in [1.0,0.5,0.05,0.005]:\n",
    "            nn = NeuralNetwork(numOfLayers=1,numOfNeurons=[1],inputSize=2,activation=['logistic'],loss='bce',lr=lrate,weights=w)\n",
    "            print(\"*\" * 50)\n",
    "            print(\"Learning Rate: %f\" % (lrate))\n",
    "            t_error = nn.train(x,y,iterations=100000)\n",
    "            plt.plot(t_error,label='Learing Rate: %f' % (lrate))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n\\nANN with two layers (one hidden & one output) ......')\n",
    "        for lrate in [1.0,0.5,0.05,0.005]:\n",
    "            nn = NeuralNetwork(numOfLayers=2,numOfNeurons=[2,1],inputSize=2,activation=['logistic','logistic'],loss='bce',lr=lrate,weights=w)\n",
    "            print(\"*\" * 50)\n",
    "            print(\"Learning Rate: %f\" % (lrate))\n",
    "            t_error = nn.train(x,y,iterations=100000)\n",
    "            plt.plot(t_error,label='Learing Rate: %f' % (lrate))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \"\"\"nn = NeuralNetwork(numOfLayers=3,numOfNeurons=[4,2,1],inputSize=2,activation=['logistic','logistic','logistic'],loss='bce',lr=0.005,weights=w)\n",
    "        t_error = nn.train(x,y,iterations=100000)\n",
    "        \n",
    "        plt.plot(t_error)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run example from class (single step)\n[0.35891648 0.40866619 0.53075072]\n[0.51130127 0.56137012 0.61904912]\n[0.14978072 0.19956143 0.34561432]\n[0.24975114 0.29950229 0.34502287]\nFinal Output:\nInput: [0.05 0.1 ] Output: [0.72844176 0.77837692]\n"
     ]
    }
   ],
   "source": [
    "print('run example from class (single step)')\n",
    "w = np.array([[[.15,.2,.35],[.25,.3,.35]],[[.4,.45,.6],[.5,.55,.6]]])\n",
    "x = np.array([[0.05,0.1]])\n",
    "y = np.array([[0.01,0.99]])\n",
    "\n",
    "nn = NeuralNetwork(numOfLayers=2,numOfNeurons=[2,2],inputSize=2,activation=['logistic','logistic'],loss='sse',lr=0.5,weights=w)\n",
    "t_error = nn.train(x,y,iterations=1,printWeight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([[[1,2,3], [4,5,6], [7,8,9]],\n",
    "            [[1,2,3], [4,5,6], [7,8,9]]])\n",
    "#c = np.hstack((b, np.ones([b.shape[0],1])))\n",
    "c = b.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1]\n [1]]\n[[2]\n [2]]\n[[3]\n [3]]\n[[4]\n [4]]\n[[5]\n [5]]\n[[6]\n [6]]\n[[7]\n [7]]\n[[8]\n [8]]\n[[9]\n [9]]\n"
     ]
    }
   ],
   "source": [
    "for i in b.reshape(-1,9).T:\n",
    "    print(i.reshape(-1,1))"
   ]
  }
 ]
}